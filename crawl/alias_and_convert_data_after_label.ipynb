{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b27e2101",
   "metadata": {},
   "source": [
    "Tạo alias cho tên người"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd694385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from unidecode import unidecode\n",
    "\n",
    "WIKI_URL = \"https://en.wikipedia.org/wiki/List_of_Vietnamese_people\"\n",
    "\n",
    "def fetch_names():\n",
    "    resp = requests.get(WIKI_URL)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    content = soup.find(\"div\", id=\"mw-content-text\")\n",
    "    names = []\n",
    "    for li in content.find_all(\"li\"):\n",
    "        if li.find_previous_sibling(lambda tag: tag.name==\"h2\" and \"See also\" in tag.text):\n",
    "            break\n",
    "        a = li.find(\"a\")\n",
    "        if a and a.get(\"href\", \"\").startswith(\"/wiki/\"):\n",
    "            name = a.text.strip()\n",
    "            if name and not name.startswith(\"List\"):\n",
    "                names.append(name)\n",
    "    return list(dict.fromkeys(names))\n",
    "\n",
    "def generate_aliases(full_name):\n",
    "    parts = full_name.split()\n",
    "    no_accents = unidecode(full_name)\n",
    "    initials = \".\".join(p[0] for p in parts) + \".\"\n",
    "    family = parts[0]\n",
    "    given = parts[-1]\n",
    "    fam_init = parts[0][0] + \".\"\n",
    "    giv_init = parts[-1][0] + \".\"\n",
    "\n",
    "    aliases = {\n",
    "        full_name,\n",
    "        no_accents,\n",
    "        initials,\n",
    "        family,\n",
    "        given,\n",
    "        f\"{fam_init} {giv_init}\",\n",
    "        f\"{family} {giv_init}\",\n",
    "        f\"{fam_init} {given}\",\n",
    "    }\n",
    "    return [a for a in sorted(aliases) if a and a != full_name]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    names = fetch_names()\n",
    "    name_dict = {}\n",
    "    for nm in names:\n",
    "        name_dict[nm] = generate_aliases(nm)\n",
    "\n",
    "    import json\n",
    "    with open(\"vietnamese_people_aliases.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(name_dict, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Đã tạo dict cho {len(name_dict)} tên.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ef7d7e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated aliases for 1052 names and saved to celebrity_aliases.json.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from unidecode import unidecode\n",
    "\n",
    "input_path = r\"E:\\Documents\\DS200\\final_project\\facebook_crawl\\list_name_celeb.txt\"\n",
    "output_path = \"celebrity_aliases.json\"\n",
    "\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    names = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "\n",
    "def generate_aliases(name):\n",
    "    variants = set()\n",
    "    no_accents = unidecode(name)\n",
    "\n",
    "    variants.update([\n",
    "        name,\n",
    "        name.lower(),\n",
    "        name.upper(),\n",
    "        no_accents,\n",
    "        no_accents.lower(),\n",
    "        no_accents.upper()\n",
    "    ])\n",
    "\n",
    "    if \"-\" in name:\n",
    "        parts = name.split(\"-\")\n",
    "        for part in parts:\n",
    "            variants.update(generate_aliases(part.strip()))\n",
    "        variants.add(\" \".join(parts))\n",
    "        variants.add(\" \".join(parts).lower())\n",
    "        variants.add(\" \".join(parts).upper())\n",
    "\n",
    "    if \" \" in name:\n",
    "        parts = name.split()\n",
    "        initials = [p[0] for p in parts if p]\n",
    "\n",
    "        if len(parts) >= 2:\n",
    "            # Viết tắt: N.T.T.T\n",
    "            dot_format = \".\".join(initials) + \".\"\n",
    "            flat = \"\".join(initials)\n",
    "            spaced = \" \".join(initials)\n",
    "\n",
    "            variants.update([\n",
    "                dot_format, dot_format.lower(), dot_format.upper(),\n",
    "                flat, flat.lower(), flat.upper(),\n",
    "                spaced, spaced.lower(), spaced.upper()\n",
    "            ])\n",
    "\n",
    "            # Tên đệm + tên\n",
    "            middle_last = \" \".join(parts[-2:])\n",
    "            variants.update([middle_last, middle_last.lower(), middle_last.upper()])\n",
    "\n",
    "            # Viết tắt đệm + tên: T.Tiên\n",
    "            ttien = f\"{parts[-2][0]}.{parts[-1]}\"\n",
    "            variants.update([ttien, ttien.lower(), ttien.upper()])\n",
    "\n",
    "        # In hoa toàn bộ cụm tên\n",
    "        all_upper = \" \".join([p.upper() for p in parts])\n",
    "        all_lower = \" \".join([p.lower() for p in parts])\n",
    "        variants.update([all_upper, all_lower])\n",
    "\n",
    "    variants.discard(\"\")\n",
    "    variants.discard(name)  # giữ bản gốc làm key, không là alias\n",
    "\n",
    "    return sorted(variants)\n",
    "\n",
    "\n",
    "alias_dict = {}\n",
    "for nm in names:\n",
    "    alias_dict[nm] = generate_aliases(nm)\n",
    "\n",
    "with open(output_path, 'w', encoding='utf-8') as outf:\n",
    "    json.dump(alias_dict, outf, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Generated aliases for {len(alias_dict)} names and saved to {output_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397042f5",
   "metadata": {},
   "source": [
    "Sau khi label bị lỗi đoạn điền label vào file json nên cần đoạn này để xóa phần thừa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d54519e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Đã tạo: data_labeled_00_cleaned.json\n",
      "✅ Đã tạo: data_labeled_01_cleaned.json\n",
      "✅ Đã tạo: data_labeled_02_cleaned.json\n",
      "✅ Đã tạo: data_labeled_03_cleaned.json\n",
      "✅ Đã tạo: data_labeled_04_cleaned.json\n",
      "✅ Đã tạo: data_labeled_05_cleaned.json\n",
      "✅ Đã tạo: data_labeled_06_cleaned.json\n",
      "✅ Đã tạo: data_labeled_07_cleaned.json\n",
      "✅ Đã tạo: data_labeled_08_cleaned.json\n",
      "✅ Đã tạo: data_labeled_09_cleaned.json\n",
      "✅ Đã tạo: data_labeled_10_cleaned.json\n",
      "✅ Đã tạo: data_labeled_11_cleaned.json\n",
      "✅ Đã tạo: data_labeled_12_cleaned.json\n",
      "✅ Đã tạo: data_labeled_13_cleaned.json\n",
      "✅ Đã tạo: data_labeled_14_cleaned.json\n",
      "✅ Đã tạo: data_labeled_15_cleaned.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Thư mục chứa các file JSON gốc\n",
    "folder = r\"E:\\Documents\\DS200\\final_project\\facebook_crawl\\data\\json\\strict\"\n",
    "\n",
    "# Duyệt qua tất cả 16 file: từ 00 đến 15\n",
    "for i in range(16):\n",
    "    original_filename = f\"data_labeled_{i:02}.json\"\n",
    "    new_filename = f\"data_labeled_{i:02}_cleaned.json\"\n",
    "\n",
    "    original_path = os.path.join(folder, original_filename)\n",
    "    new_path = os.path.join(folder, new_filename)\n",
    "\n",
    "    with open(original_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Cập nhật từng post trong danh sách\n",
    "    for post in data:\n",
    "        # Xóa các trường không cần thiết trong comment\n",
    "        for key in [\"Persons\", \"Aspect_1\", \"Aspect_2\", \"Sentiment\"]:\n",
    "            post.get(\"comment\", {}).pop(key, None)\n",
    "\n",
    "        # Thêm trường Persons mới ở cấp ngoài nếu chưa có\n",
    "        if \"Persons\" not in post:\n",
    "            post[\"Persons\"] = []\n",
    "\n",
    "    # Ghi ra file mới để so sánh / backup\n",
    "    with open(new_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"✅ Đã tạo: {new_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80217da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Đã nạp: data_labeled_00_cleaned.json (374 mục)\n",
      "📥 Đã nạp: data_labeled_01_cleaned.json (374 mục)\n",
      "📥 Đã nạp: data_labeled_02_cleaned.json (374 mục)\n",
      "📥 Đã nạp: data_labeled_03_cleaned.json (374 mục)\n",
      "📥 Đã nạp: data_labeled_04_cleaned.json (374 mục)\n",
      "📥 Đã nạp: data_labeled_05_cleaned.json (374 mục)\n",
      "📥 Đã nạp: data_labeled_06_cleaned.json (374 mục)\n",
      "📥 Đã nạp: data_labeled_07_cleaned.json (374 mục)\n",
      "📥 Đã nạp: data_labeled_08_cleaned.json (374 mục)\n",
      "📥 Đã nạp: data_labeled_09_cleaned.json (374 mục)\n",
      "📥 Đã nạp: data_labeled_10_cleaned.json (374 mục)\n",
      "📥 Đã nạp: data_labeled_11_cleaned.json (374 mục)\n",
      "📥 Đã nạp: data_labeled_12_cleaned.json (374 mục)\n",
      "📥 Đã nạp: data_labeled_13_cleaned.json (374 mục)\n",
      "📥 Đã nạp: data_labeled_14_cleaned.json (374 mục)\n",
      "📥 Đã nạp: data_labeled_15_cleaned.json (362 mục)\n",
      "\n",
      "✅ Đã tạo file tổng: E:\\Documents\\DS200\\final_project\\facebook_crawl\\data\\json\\data_labeled_strict.json (5972 mục)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Thư mục chứa các file cleaned\n",
    "folder = r\"E:\\Documents\\DS200\\final_project\\facebook_crawl\\data\\json\\strict\"\n",
    "folder_output = r\"E:\\Documents\\DS200\\final_project\\facebook_crawl\\data\\json\"\n",
    "output_file = os.path.join(folder_output, \"data_labeled_strict.json\")\n",
    "\n",
    "# Khởi tạo danh sách để chứa toàn bộ dữ liệu\n",
    "all_data = []\n",
    "\n",
    "# Duyệt qua tất cả các file cleaned từ 00 đến 15\n",
    "for i in range(16):\n",
    "    cleaned_filename = f\"data_labeled_{i:02}_cleaned.json\"\n",
    "    cleaned_path = os.path.join(folder, cleaned_filename)\n",
    "\n",
    "    # Đọc dữ liệu và thêm vào danh sách chung\n",
    "    with open(cleaned_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        part_data = json.load(f)\n",
    "        all_data.extend(part_data)\n",
    "\n",
    "    print(f\"📥 Đã nạp: {cleaned_filename} ({len(part_data)} mục)\")\n",
    "\n",
    "# Ghi toàn bộ vào file tổng\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n✅ Đã tạo file tổng: {output_file} ({len(all_data)} mục)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c5f018c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Đã chuẩn hóa và lưu vào: E:\\Documents\\DS200\\final_project\\facebook_crawl\\data\\json\\data_labeled_strict_cleaned.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Đường dẫn file gốc\n",
    "folder = r\"E:\\Documents\\DS200\\final_project\\facebook_crawl\\data\\json\"\n",
    "input_file = os.path.join(folder, \"data_labeled_strict.json\")\n",
    "output_file = os.path.join(folder, \"data_labeled_strict_cleaned.json\")\n",
    "\n",
    "def aspect_priority(a):\n",
    "    if a == \"null\":\n",
    "        return 0\n",
    "    elif a == \"Other\":\n",
    "        return 1\n",
    "    return 2\n",
    "\n",
    "# Tải dữ liệu\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Duyệt và chuẩn hóa từng bản ghi\n",
    "for post in data:\n",
    "    a1 = str(post.get(\"Aspect_1\", \"null\") or \"null\")\n",
    "    a2 = str(post.get(\"Aspect_2\", \"null\") or \"null\")\n",
    "    sentiments = post.get(\"Sentiment\", [\"null\", \"null\"])\n",
    "\n",
    "    # Đảm bảo sentiment có đủ 2 phần tử\n",
    "    if len(sentiments) < 2:\n",
    "        sentiments += [\"null\"] * (2 - len(sentiments))\n",
    "    sentiments = [s if s else \"null\" for s in sentiments]\n",
    "\n",
    "    # Nếu chỉ có 1 aspect hợp lệ (aspect cụ thể hoặc \"Other\"), đẩy lên Aspect_1\n",
    "    if a1 == \"null\" and a2 != \"null\":\n",
    "        a1, a2 = a2, \"null\"\n",
    "        sentiments[0], sentiments[1] = sentiments[1], \"null\"\n",
    "\n",
    "    elif a1 != \"null\" and a2 == \"null\":\n",
    "        sentiments[1] = \"null\"\n",
    "\n",
    "    # Nếu cả hai hợp lệ thì sắp xếp theo ưu tiên\n",
    "    elif a1 != \"null\" and a2 != \"null\":\n",
    "        if aspect_priority(a2) > aspect_priority(a1):\n",
    "            a1, a2 = a2, a1\n",
    "            sentiments[0], sentiments[1] = sentiments[1], sentiments[0]\n",
    "\n",
    "    # Đảm bảo nếu a2 là \"null\" thì sentiment[1] cũng là \"null\"\n",
    "    if a2 == \"null\":\n",
    "        sentiments[1] = \"null\"\n",
    "\n",
    "    # Ghi lại vào post\n",
    "    post[\"Aspect_1\"] = a1\n",
    "    post[\"Aspect_2\"] = a2\n",
    "    post[\"Sentiment\"] = sentiments\n",
    "\n",
    "# Ghi ra file mới\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"✅ Đã chuẩn hóa và lưu vào: {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
