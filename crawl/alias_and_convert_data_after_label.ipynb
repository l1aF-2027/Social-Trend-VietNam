{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b27e2101",
   "metadata": {},
   "source": [
    "Táº¡o alias cho tÃªn ngÆ°á»i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd694385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from unidecode import unidecode\n",
    "\n",
    "WIKI_URL = \"https://en.wikipedia.org/wiki/List_of_Vietnamese_people\"\n",
    "\n",
    "def fetch_names():\n",
    "    resp = requests.get(WIKI_URL)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    content = soup.find(\"div\", id=\"mw-content-text\")\n",
    "    names = []\n",
    "    for li in content.find_all(\"li\"):\n",
    "        if li.find_previous_sibling(lambda tag: tag.name==\"h2\" and \"See also\" in tag.text):\n",
    "            break\n",
    "        a = li.find(\"a\")\n",
    "        if a and a.get(\"href\", \"\").startswith(\"/wiki/\"):\n",
    "            name = a.text.strip()\n",
    "            if name and not name.startswith(\"List\"):\n",
    "                names.append(name)\n",
    "    return list(dict.fromkeys(names))\n",
    "\n",
    "def generate_aliases(full_name):\n",
    "    parts = full_name.split()\n",
    "    no_accents = unidecode(full_name)\n",
    "    initials = \".\".join(p[0] for p in parts) + \".\"\n",
    "    family = parts[0]\n",
    "    given = parts[-1]\n",
    "    fam_init = parts[0][0] + \".\"\n",
    "    giv_init = parts[-1][0] + \".\"\n",
    "\n",
    "    aliases = {\n",
    "        full_name,\n",
    "        no_accents,\n",
    "        initials,\n",
    "        family,\n",
    "        given,\n",
    "        f\"{fam_init} {giv_init}\",\n",
    "        f\"{family} {giv_init}\",\n",
    "        f\"{fam_init} {given}\",\n",
    "    }\n",
    "    return [a for a in sorted(aliases) if a and a != full_name]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    names = fetch_names()\n",
    "    name_dict = {}\n",
    "    for nm in names:\n",
    "        name_dict[nm] = generate_aliases(nm)\n",
    "\n",
    "    import json\n",
    "    with open(\"vietnamese_people_aliases.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(name_dict, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"ÄÃ£ táº¡o dict cho {len(name_dict)} tÃªn.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ef7d7e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated aliases for 1052 names and saved to celebrity_aliases.json.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from unidecode import unidecode\n",
    "\n",
    "input_path = r\"E:\\Documents\\DS200\\final_project\\facebook_crawl\\list_name_celeb.txt\"\n",
    "output_path = \"celebrity_aliases.json\"\n",
    "\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    names = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "\n",
    "def generate_aliases(name):\n",
    "    variants = set()\n",
    "    no_accents = unidecode(name)\n",
    "\n",
    "    variants.update([\n",
    "        name,\n",
    "        name.lower(),\n",
    "        name.upper(),\n",
    "        no_accents,\n",
    "        no_accents.lower(),\n",
    "        no_accents.upper()\n",
    "    ])\n",
    "\n",
    "    if \"-\" in name:\n",
    "        parts = name.split(\"-\")\n",
    "        for part in parts:\n",
    "            variants.update(generate_aliases(part.strip()))\n",
    "        variants.add(\" \".join(parts))\n",
    "        variants.add(\" \".join(parts).lower())\n",
    "        variants.add(\" \".join(parts).upper())\n",
    "\n",
    "    if \" \" in name:\n",
    "        parts = name.split()\n",
    "        initials = [p[0] for p in parts if p]\n",
    "\n",
    "        if len(parts) >= 2:\n",
    "            # Viáº¿t táº¯t: N.T.T.T\n",
    "            dot_format = \".\".join(initials) + \".\"\n",
    "            flat = \"\".join(initials)\n",
    "            spaced = \" \".join(initials)\n",
    "\n",
    "            variants.update([\n",
    "                dot_format, dot_format.lower(), dot_format.upper(),\n",
    "                flat, flat.lower(), flat.upper(),\n",
    "                spaced, spaced.lower(), spaced.upper()\n",
    "            ])\n",
    "\n",
    "            # TÃªn Ä‘á»‡m + tÃªn\n",
    "            middle_last = \" \".join(parts[-2:])\n",
    "            variants.update([middle_last, middle_last.lower(), middle_last.upper()])\n",
    "\n",
    "            # Viáº¿t táº¯t Ä‘á»‡m + tÃªn: T.TiÃªn\n",
    "            ttien = f\"{parts[-2][0]}.{parts[-1]}\"\n",
    "            variants.update([ttien, ttien.lower(), ttien.upper()])\n",
    "\n",
    "        # In hoa toÃ n bá»™ cá»¥m tÃªn\n",
    "        all_upper = \" \".join([p.upper() for p in parts])\n",
    "        all_lower = \" \".join([p.lower() for p in parts])\n",
    "        variants.update([all_upper, all_lower])\n",
    "\n",
    "    variants.discard(\"\")\n",
    "    variants.discard(name)  # giá»¯ báº£n gá»‘c lÃ m key, khÃ´ng lÃ  alias\n",
    "\n",
    "    return sorted(variants)\n",
    "\n",
    "\n",
    "alias_dict = {}\n",
    "for nm in names:\n",
    "    alias_dict[nm] = generate_aliases(nm)\n",
    "\n",
    "with open(output_path, 'w', encoding='utf-8') as outf:\n",
    "    json.dump(alias_dict, outf, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Generated aliases for {len(alias_dict)} names and saved to {output_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397042f5",
   "metadata": {},
   "source": [
    "Sau khi label bá»‹ lá»—i Ä‘oáº¡n Ä‘iá»n label vÃ o file json nÃªn cáº§n Ä‘oáº¡n nÃ y Ä‘á»ƒ xÃ³a pháº§n thá»«a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d54519e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ÄÃ£ táº¡o: data_labeled_00_cleaned.json\n",
      "âœ… ÄÃ£ táº¡o: data_labeled_01_cleaned.json\n",
      "âœ… ÄÃ£ táº¡o: data_labeled_02_cleaned.json\n",
      "âœ… ÄÃ£ táº¡o: data_labeled_03_cleaned.json\n",
      "âœ… ÄÃ£ táº¡o: data_labeled_04_cleaned.json\n",
      "âœ… ÄÃ£ táº¡o: data_labeled_05_cleaned.json\n",
      "âœ… ÄÃ£ táº¡o: data_labeled_06_cleaned.json\n",
      "âœ… ÄÃ£ táº¡o: data_labeled_07_cleaned.json\n",
      "âœ… ÄÃ£ táº¡o: data_labeled_08_cleaned.json\n",
      "âœ… ÄÃ£ táº¡o: data_labeled_09_cleaned.json\n",
      "âœ… ÄÃ£ táº¡o: data_labeled_10_cleaned.json\n",
      "âœ… ÄÃ£ táº¡o: data_labeled_11_cleaned.json\n",
      "âœ… ÄÃ£ táº¡o: data_labeled_12_cleaned.json\n",
      "âœ… ÄÃ£ táº¡o: data_labeled_13_cleaned.json\n",
      "âœ… ÄÃ£ táº¡o: data_labeled_14_cleaned.json\n",
      "âœ… ÄÃ£ táº¡o: data_labeled_15_cleaned.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# ThÆ° má»¥c chá»©a cÃ¡c file JSON gá»‘c\n",
    "folder = r\"E:\\Documents\\DS200\\final_project\\facebook_crawl\\data\\json\\strict\"\n",
    "\n",
    "# Duyá»‡t qua táº¥t cáº£ 16 file: tá»« 00 Ä‘áº¿n 15\n",
    "for i in range(16):\n",
    "    original_filename = f\"data_labeled_{i:02}.json\"\n",
    "    new_filename = f\"data_labeled_{i:02}_cleaned.json\"\n",
    "\n",
    "    original_path = os.path.join(folder, original_filename)\n",
    "    new_path = os.path.join(folder, new_filename)\n",
    "\n",
    "    with open(original_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Cáº­p nháº­t tá»«ng post trong danh sÃ¡ch\n",
    "    for post in data:\n",
    "        # XÃ³a cÃ¡c trÆ°á»ng khÃ´ng cáº§n thiáº¿t trong comment\n",
    "        for key in [\"Persons\", \"Aspect_1\", \"Aspect_2\", \"Sentiment\"]:\n",
    "            post.get(\"comment\", {}).pop(key, None)\n",
    "\n",
    "        # ThÃªm trÆ°á»ng Persons má»›i á»Ÿ cáº¥p ngoÃ i náº¿u chÆ°a cÃ³\n",
    "        if \"Persons\" not in post:\n",
    "            post[\"Persons\"] = []\n",
    "\n",
    "    # Ghi ra file má»›i Ä‘á»ƒ so sÃ¡nh / backup\n",
    "    with open(new_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"âœ… ÄÃ£ táº¡o: {new_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80217da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ ÄÃ£ náº¡p: data_labeled_00_cleaned.json (374 má»¥c)\n",
      "ğŸ“¥ ÄÃ£ náº¡p: data_labeled_01_cleaned.json (374 má»¥c)\n",
      "ğŸ“¥ ÄÃ£ náº¡p: data_labeled_02_cleaned.json (374 má»¥c)\n",
      "ğŸ“¥ ÄÃ£ náº¡p: data_labeled_03_cleaned.json (374 má»¥c)\n",
      "ğŸ“¥ ÄÃ£ náº¡p: data_labeled_04_cleaned.json (374 má»¥c)\n",
      "ğŸ“¥ ÄÃ£ náº¡p: data_labeled_05_cleaned.json (374 má»¥c)\n",
      "ğŸ“¥ ÄÃ£ náº¡p: data_labeled_06_cleaned.json (374 má»¥c)\n",
      "ğŸ“¥ ÄÃ£ náº¡p: data_labeled_07_cleaned.json (374 má»¥c)\n",
      "ğŸ“¥ ÄÃ£ náº¡p: data_labeled_08_cleaned.json (374 má»¥c)\n",
      "ğŸ“¥ ÄÃ£ náº¡p: data_labeled_09_cleaned.json (374 má»¥c)\n",
      "ğŸ“¥ ÄÃ£ náº¡p: data_labeled_10_cleaned.json (374 má»¥c)\n",
      "ğŸ“¥ ÄÃ£ náº¡p: data_labeled_11_cleaned.json (374 má»¥c)\n",
      "ğŸ“¥ ÄÃ£ náº¡p: data_labeled_12_cleaned.json (374 má»¥c)\n",
      "ğŸ“¥ ÄÃ£ náº¡p: data_labeled_13_cleaned.json (374 má»¥c)\n",
      "ğŸ“¥ ÄÃ£ náº¡p: data_labeled_14_cleaned.json (374 má»¥c)\n",
      "ğŸ“¥ ÄÃ£ náº¡p: data_labeled_15_cleaned.json (362 má»¥c)\n",
      "\n",
      "âœ… ÄÃ£ táº¡o file tá»•ng: E:\\Documents\\DS200\\final_project\\facebook_crawl\\data\\json\\data_labeled_strict.json (5972 má»¥c)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# ThÆ° má»¥c chá»©a cÃ¡c file cleaned\n",
    "folder = r\"E:\\Documents\\DS200\\final_project\\facebook_crawl\\data\\json\\strict\"\n",
    "folder_output = r\"E:\\Documents\\DS200\\final_project\\facebook_crawl\\data\\json\"\n",
    "output_file = os.path.join(folder_output, \"data_labeled_strict.json\")\n",
    "\n",
    "# Khá»Ÿi táº¡o danh sÃ¡ch Ä‘á»ƒ chá»©a toÃ n bá»™ dá»¯ liá»‡u\n",
    "all_data = []\n",
    "\n",
    "# Duyá»‡t qua táº¥t cáº£ cÃ¡c file cleaned tá»« 00 Ä‘áº¿n 15\n",
    "for i in range(16):\n",
    "    cleaned_filename = f\"data_labeled_{i:02}_cleaned.json\"\n",
    "    cleaned_path = os.path.join(folder, cleaned_filename)\n",
    "\n",
    "    # Äá»c dá»¯ liá»‡u vÃ  thÃªm vÃ o danh sÃ¡ch chung\n",
    "    with open(cleaned_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        part_data = json.load(f)\n",
    "        all_data.extend(part_data)\n",
    "\n",
    "    print(f\"ğŸ“¥ ÄÃ£ náº¡p: {cleaned_filename} ({len(part_data)} má»¥c)\")\n",
    "\n",
    "# Ghi toÃ n bá»™ vÃ o file tá»•ng\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nâœ… ÄÃ£ táº¡o file tá»•ng: {output_file} ({len(all_data)} má»¥c)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c5f018c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ÄÃ£ chuáº©n hÃ³a vÃ  lÆ°u vÃ o: E:\\Documents\\DS200\\final_project\\facebook_crawl\\data\\json\\data_labeled_strict_cleaned.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# ÄÆ°á»ng dáº«n file gá»‘c\n",
    "folder = r\"E:\\Documents\\DS200\\final_project\\facebook_crawl\\data\\json\"\n",
    "input_file = os.path.join(folder, \"data_labeled_strict.json\")\n",
    "output_file = os.path.join(folder, \"data_labeled_strict_cleaned.json\")\n",
    "\n",
    "def aspect_priority(a):\n",
    "    if a == \"null\":\n",
    "        return 0\n",
    "    elif a == \"Other\":\n",
    "        return 1\n",
    "    return 2\n",
    "\n",
    "# Táº£i dá»¯ liá»‡u\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Duyá»‡t vÃ  chuáº©n hÃ³a tá»«ng báº£n ghi\n",
    "for post in data:\n",
    "    a1 = str(post.get(\"Aspect_1\", \"null\") or \"null\")\n",
    "    a2 = str(post.get(\"Aspect_2\", \"null\") or \"null\")\n",
    "    sentiments = post.get(\"Sentiment\", [\"null\", \"null\"])\n",
    "\n",
    "    # Äáº£m báº£o sentiment cÃ³ Ä‘á»§ 2 pháº§n tá»­\n",
    "    if len(sentiments) < 2:\n",
    "        sentiments += [\"null\"] * (2 - len(sentiments))\n",
    "    sentiments = [s if s else \"null\" for s in sentiments]\n",
    "\n",
    "    # Náº¿u chá»‰ cÃ³ 1 aspect há»£p lá»‡ (aspect cá»¥ thá»ƒ hoáº·c \"Other\"), Ä‘áº©y lÃªn Aspect_1\n",
    "    if a1 == \"null\" and a2 != \"null\":\n",
    "        a1, a2 = a2, \"null\"\n",
    "        sentiments[0], sentiments[1] = sentiments[1], \"null\"\n",
    "\n",
    "    elif a1 != \"null\" and a2 == \"null\":\n",
    "        sentiments[1] = \"null\"\n",
    "\n",
    "    # Náº¿u cáº£ hai há»£p lá»‡ thÃ¬ sáº¯p xáº¿p theo Æ°u tiÃªn\n",
    "    elif a1 != \"null\" and a2 != \"null\":\n",
    "        if aspect_priority(a2) > aspect_priority(a1):\n",
    "            a1, a2 = a2, a1\n",
    "            sentiments[0], sentiments[1] = sentiments[1], sentiments[0]\n",
    "\n",
    "    # Äáº£m báº£o náº¿u a2 lÃ  \"null\" thÃ¬ sentiment[1] cÅ©ng lÃ  \"null\"\n",
    "    if a2 == \"null\":\n",
    "        sentiments[1] = \"null\"\n",
    "\n",
    "    # Ghi láº¡i vÃ o post\n",
    "    post[\"Aspect_1\"] = a1\n",
    "    post[\"Aspect_2\"] = a2\n",
    "    post[\"Sentiment\"] = sentiments\n",
    "\n",
    "# Ghi ra file má»›i\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ… ÄÃ£ chuáº©n hÃ³a vÃ  lÆ°u vÃ o: {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
